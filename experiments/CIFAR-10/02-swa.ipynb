{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e3f77c-248a-4025-9fc7-02d1f8cd4aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "seed: 2246400\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torchvision.utils import make_grid\n",
    "from torch.optim.swa_utils import SWALR\n",
    "\n",
    "from swadist.data import get_dataloaders\n",
    "from swadist.train import Trainer\n",
    "from swadist.optim import LinearPolyLR\n",
    "from swadist.models import ResNet\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "print(f'Using {device}')\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "seed = int((datetime.date.today() - datetime.date(2022, 4, 11)).total_seconds())\n",
    "print(f'seed: {seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ad42e-945d-42ed-bf28-0bea96dd0237",
   "metadata": {},
   "source": [
    "### Two-phase training: SGD + SWA\n",
    "\n",
    "We train ResNet-8, using the optimal hyperparameters given in [Shallue et al. 2019](http://arxiv.org/abs/1811.03600) for phase 1 (SGD w/ Nesterov momentum). \n",
    "\n",
    "For the SWA phase, we follow [Izmailov et al.](http://arxiv.org/abs/1803.05407) and use a constant learning rate that is 10x smaller than the initial learning rate in the first phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e805248e-1f34-4d3c-96f2-2879e9c8971f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RandomSampler\n",
      "Number of training samples: 45000\n",
      "Number of training batches: 176\n",
      "\n",
      "Starting 15-epoch training loop...\n",
      "Random seed: 10951562873798627444\n",
      "\n",
      "SGD epochs: 10 | Codistillation epochs: 0 | SWA epochs: 5\n",
      "DistributedDataParallel: False\n",
      "Stopping accuracy: None\n",
      "\n",
      "Train epoch: 1 | Metrics (epoch mean): cross_entropy=1.989805 <> acc=0.255346 | Batch (size 200): 176/176 (100%) | Total steps: 176\n",
      "Validation (batch mean) |  cross_entropy=1.670629 <> accuracy=0.379216 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 2 | Metrics (epoch mean): cross_entropy=1.528005 <> acc=0.440542 | Batch (size 200): 176/176 (100%) | Total steps: 352\n",
      "Validation (batch mean) |  cross_entropy=1.420598 <> accuracy=0.486340 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 3 | Metrics (epoch mean): cross_entropy=1.252594 <> acc=0.553341 | Batch (size 200): 176/176 (100%) | Total steps: 528\n",
      "Validation (batch mean) |  cross_entropy=1.137977 <> accuracy=0.596381 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 4 | Metrics (epoch mean): cross_entropy=1.070005 <> acc=0.621490 | Batch (size 200): 176/176 (100%) | Total steps: 704\n",
      "Validation (batch mean) |  cross_entropy=1.016674 <> accuracy=0.645071 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 5 | Metrics (epoch mean): cross_entropy=0.950535 <> acc=0.666525 | Batch (size 200): 176/176 (100%) | Total steps: 880\n",
      "Validation (batch mean) |  cross_entropy=0.924353 <> accuracy=0.678964 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 6 | Metrics (epoch mean): cross_entropy=0.862649 <> acc=0.697928 | Batch (size 200): 176/176 (100%) | Total steps: 1056\n",
      "Validation (batch mean) |  cross_entropy=0.882446 <> accuracy=0.700551 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 7 | Metrics (epoch mean): cross_entropy=0.784113 <> acc=0.725371 | Batch (size 200): 176/176 (100%) | Total steps: 1232\n",
      "Validation (batch mean) |  cross_entropy=0.823342 <> accuracy=0.722185 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 8 | Metrics (epoch mean): cross_entropy=0.718812 <> acc=0.749395 | Batch (size 200): 176/176 (100%) | Total steps: 1408\n",
      "Validation (batch mean) |  cross_entropy=0.809607 <> accuracy=0.723874 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 9 | Metrics (epoch mean): cross_entropy=0.657084 <> acc=0.771217 | Batch (size 200): 176/176 (100%) | Total steps: 1584\n",
      "Validation (batch mean) |  cross_entropy=0.787279 <> accuracy=0.730584 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 10 | Metrics (epoch mean): cross_entropy=0.591895 <> acc=0.792946 | Batch (size 200): 176/176 (100%) | Total steps: 1760\n",
      "Validation (batch mean) |  cross_entropy=0.767916 <> accuracy=0.744072 | Batch: 20/20 (100%)\n",
      "\n",
      "Starting SWA phase...\n",
      "\n",
      "Train epoch: 11 | Metrics (epoch mean): cross_entropy=0.539127 <> acc=0.810257 | Batch (size 200): 176/176 (100%) | Total steps: 1936\n",
      "Validation (batch mean) |  cross_entropy=0.735259 <> accuracy=0.756399 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 12 | Metrics (epoch mean): cross_entropy=0.479516 <> acc=0.831950 | Batch (size 200): 176/176 (100%) | Total steps: 2112\n",
      "Validation (batch mean) |  cross_entropy=0.721534 <> accuracy=0.761799 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 13 | Metrics (epoch mean): cross_entropy=0.386671 <> acc=0.866747 | Batch (size 200): 176/176 (100%) | Total steps: 2288\n",
      "Validation (batch mean) |  cross_entropy=0.722134 <> accuracy=0.764729 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 14 | Metrics (epoch mean): cross_entropy=0.321272 <> acc=0.889738 | Batch (size 200): 176/176 (100%) | Total steps: 2464\n",
      "Validation (batch mean) |  cross_entropy=0.728802 <> accuracy=0.767682 | Batch: 20/20 (100%)\n",
      "\n",
      "Train epoch: 15 | Metrics (epoch mean): cross_entropy=0.294962 <> acc=0.901741 | Batch (size 200): 176/176 (100%) | Total steps: 2640\n",
      "Validation (batch mean) |  cross_entropy=0.738952 <> accuracy=0.766314 | Batch: 20/20 (100%)\n",
      "\n",
      "CPU times: user 2min 58s, sys: 14.1 s, total: 3min 12s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# whether to log training to Tensorboard\n",
    "log = False\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# optimizer\n",
    "lr0, momentum,  = 2**-5., 0.975\n",
    "\n",
    "# scheduler\n",
    "alpha, decay_epochs = 0.25, 15\n",
    "\n",
    "# swa_scheduler\n",
    "swa_lr = lr0 / 10\n",
    "\n",
    "# training epochs\n",
    "epochs_sgd, epochs_swa = 10, 5\n",
    "\n",
    "# loaders\n",
    "train_loader, valid_loader = get_dataloaders(dataset=\"cifar10\", \n",
    "                                             batch_size=batch_size,\n",
    "                                             num_workers=4,  \n",
    "                                             test=False,\n",
    "                                             pin_memory=cuda)\n",
    "\n",
    "# model\n",
    "resnet8 = ResNet(in_kernel_size=3, \n",
    "                 stack_sizes=[1, 1, 1], \n",
    "                 n_classes=10, \n",
    "                 batch_norm=False).to(device)\n",
    "\n",
    "# optimizer and schedulers\n",
    "optimizer = torch.optim.SGD(resnet8.parameters(), \n",
    "                            lr=lr0, \n",
    "                            momentum=momentum, \n",
    "                            nesterov=True)\n",
    "\n",
    "scheduler = LinearPolyLR(optimizer, \n",
    "                         alpha=alpha, \n",
    "                         decay_epochs=decay_epochs)\n",
    "\n",
    "# instantiating the SWALR too early seems to have a negative effect on SGD phase (?)\n",
    "# swa_scheduler = SWALR(optimizer, \n",
    "#                       swa_lr=swa_lr, \n",
    "#                       anneal_strategy='linear', \n",
    "#                       anneal_epochs=0)\n",
    "\n",
    "swalr_kwargs = {\n",
    "    'swa_lr': swa_lr, \n",
    "    'anneal_strategy': 'cos', \n",
    "    'anneal_epochs': 3\n",
    "}\n",
    "\n",
    "trainer = Trainer(resnet8, \n",
    "                  train_loader, \n",
    "                  valid_loader, \n",
    "                  F.cross_entropy, \n",
    "                  optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  # swa_scheduler=swa_scheduler,\n",
    "                  swa_scheduler=swalr_kwargs,\n",
    "                  device=device,\n",
    "                  name='swa',\n",
    "                  log=log)\n",
    "\n",
    "# begin training\n",
    "trainer(epochs_sgd=epochs_sgd, \n",
    "        epochs_swa=epochs_swa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2731bc2f-e1f2-4c32-88fd-b25a04d7ab2c",
   "metadata": {},
   "source": [
    "### Validation accuracy by target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd6ead-5724-40a1-a15f-d5f86f3e078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "labels = []\n",
    "preds = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for data in valid_loader:\n",
    "        images, target = data\n",
    "        labels.append(target.numpy())\n",
    "        outputs = resnet8(images.to(device)).cpu()\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        preds.append(predictions)\n",
    "        total += target.size(0)\n",
    "        correct += (predictions == target).sum().item()\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(target, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "print(f'Validation accuracy: {100 * correct // total}%')\n",
    "            \n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "\n",
    "labels = pd.Series(np.hstack(labels).astype(int), name=\"Labels\")\n",
    "preds = pd.Series(np.hstack(preds).astype(int), name=\"Preds\")\n",
    "df_confusion = pd.crosstab(classes[labels], classes[preds])\n",
    "df_confusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 [conda:torch]",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
